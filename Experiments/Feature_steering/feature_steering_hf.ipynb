{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:02:50.010183Z",
     "start_time": "2024-07-22T20:02:43.397665Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 设置环境变量\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('D:\\ComputerScience\\Research\\PRADA\\sparse_autoencoder')\n",
    "# os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "# 导入库\n",
    "import torch\n",
    "import blobfile as bf\n",
    "from experiments.utils import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2LMHeadModel, AutoTokenizer, GPT2Tokenizer, GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9f3b4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_steering(autoencoder,x: torch.Tensor, feature_indices: list[int], feature_values: list[float]) -> torch.Tensor:\n",
    "    assert len(feature_indices) == len(feature_values), \"Feature indices and values must have the same length.\"\n",
    "    feature_values = [max(min(value, 10), -10) for value in feature_values]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 获取原始特征表示和信息\n",
    "        latents, info = autoencoder.encode(x)\n",
    "        # 修改特征表示\n",
    "        for index, value in zip(feature_indices, feature_values):\n",
    "            print(\"original:\", latents[:, index])\n",
    "            if value > 0:\n",
    "                latents[:, index] *= value\n",
    "            else:\n",
    "                latents[:, index] = latents[:, index] / abs(value)\n",
    "            print(\"Modified:\", latents[:, index])\n",
    "            print(f\"Feature {index} modified with {'+' if value >= 0 else ''}{value}\")\n",
    "        # 使用修改后的特征表示通过解码器生成重构输出\n",
    "        modified_output = autoencoder.decode(latents, info)\n",
    "    return modified_output\n",
    "\n",
    "def calculate_error(input_tensor, reconstructed_activations) -> torch.Tensor:\n",
    "    # 计算误差\n",
    "    error = input_tensor - reconstructed_activations\n",
    "    # 可以选择使用不同的误差度量方式，这里使用均方误差（MSE）\n",
    "    normalized_mse = (reconstructed_activations - input_tensor).pow(2).sum(dim=1) / (input_tensor).pow(2).sum(dim=1)\n",
    "    return normalized_mse, error\n",
    "\n",
    "\n",
    "def compare_activations(tensor1, tensor2):\n",
    "    difference = tensor1 - tensor2\n",
    "    print(\"Difference between tensors:\\n\", difference)\n",
    "\n",
    "    # 计算差异的统计信息\n",
    "    mean_diff = torch.mean(difference)\n",
    "    std_diff = torch.std(difference)\n",
    "    print(f\"Mean difference: {mean_diff.item()}\")\n",
    "    print(f\"Standard deviation of difference: {std_diff.item()}\")\n",
    "\n",
    "    # 可视化差异\n",
    "    difference_np = difference.numpy()\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.imshow(difference_np, cmap='coolwarm', aspect='auto')\n",
    "    plt.colorbar(label='Difference')\n",
    "    plt.title('Difference between Reconstructed Activations and Modified Output')\n",
    "    plt.xlabel('Feature Index')\n",
    "    plt.ylabel('Sample Index')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a5bb1aac7ca685e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:03:05.660715Z",
     "start_time": "2024-07-22T20:02:50.012200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current sae:az://openaipublic/sparse-autoencoder/gpt2-small/resid_post_mlp_v5_128k/autoencoders/6.pt\n"
     ]
    }
   ],
   "source": [
    "model, auto_tokenizer, device = load_model_hf(\"gpt2\")\n",
    "layer_index = 6\n",
    "location = \"resid_post_mlp\"\n",
    "autoencoder = load_autoencoder(location, layer_index, device, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b23a7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: tensor([3.6126e-05, 3.3759e-02, 5.5583e-02, 5.5487e-02, 6.2234e-02])\n",
      "Error Tensor: tensor([[ 0.6114,  0.2868,  0.3416,  ...,  0.1430, -0.0825,  0.4055],\n",
      "        [ 0.5793,  0.8852,  0.1662,  ..., -1.2331, -0.4440, -0.5244],\n",
      "        [-0.1159, -0.9248,  0.0317,  ..., -0.9406,  0.9031, -0.0258],\n",
      "        [ 2.0144,  0.0149, -0.5361,  ...,  0.1203,  1.0314, -0.3879],\n",
      "        [ 0.0770,  0.7668, -0.1243,  ..., -1.3477,  0.1813,  1.1257]])\n",
      "torch.Size([5, 768])\n"
     ]
    }
   ],
   "source": [
    "# 计算误差\n",
    "mse_error, error = calculate_error(activation, recon_activations)\n",
    "print(\"MSE:\", mse_error)\n",
    "print(\"Error Tensor:\", error)\n",
    "print(error.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ebf18",
   "metadata": {},
   "source": [
    "# Activation Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19b49ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens ID (AutoTokenizer): tensor([[ 8491,   345, 18951, 13658,    30]])\n",
      "Tokens String (AutoTokenizer): ['Are', 'Ġyou', 'Ġintro', 'verted', '?']\n",
      "13\n",
      "resid_post_mlp for layer 6: torch.Size([5, 768])\n",
      "tensor([[ 0.8602,  0.1959,  0.4753,  ..., -1.8215, -0.2200,  0.4110],\n",
      "        [-0.5927,  0.9537, -2.2768,  ..., -1.5256, -2.6308,  1.6227],\n",
      "        [ 3.2329, -4.1209, -2.8176,  ..., -3.9215,  1.4423, -1.0927],\n",
      "        [ 4.3366,  0.8113, -2.9855,  ..., -1.3056,  5.1563,  0.3707],\n",
      "        [ 1.3002,  1.2545, -1.8011,  ..., -3.2269, -0.4410,  2.9988]])\n",
      "original: tensor([0.0000, 0.0000, 4.0403, 7.6410, 0.0000])\n",
      "Modified: tensor([0.0000, 0.0000, 0.4040, 0.7641, 0.0000])\n",
      "Feature 53912 modified with -10\n",
      "orginal modified_recon_activations: tensor([[ 0.2488, -0.0908,  0.1337,  ..., -1.9644, -0.1375,  0.0055],\n",
      "        [-1.1720,  0.0685, -2.4429,  ..., -0.2925, -2.1868,  2.1471],\n",
      "        [ 2.9070, -2.8697, -2.8445,  ..., -2.9103, -0.7883, -1.6302],\n",
      "        [ 1.4481,  1.4420, -2.4400,  ..., -1.2863,  1.4986, -0.3558],\n",
      "        [ 1.2232,  0.4878, -1.6768,  ..., -1.8792, -0.6223,  1.8731]])\n",
      "torch.Size([5, 768])\n",
      "modified_recon_activations + error: tensor([[ 0.8602,  0.1959,  0.4753,  ..., -1.8215, -0.2200,  0.4110],\n",
      "        [-0.5927,  0.9537, -2.2768,  ..., -1.5256, -2.6308,  1.6227],\n",
      "        [ 2.7911, -3.7945, -2.8128,  ..., -3.8509,  0.1147, -1.6560],\n",
      "        [ 3.4625,  1.4570, -2.9760,  ..., -1.1660,  2.5300, -0.7437],\n",
      "        [ 1.3002,  1.2545, -1.8011,  ..., -3.2269, -0.4410,  2.9988]])\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.4418, -0.3264, -0.0048,  ..., -0.0706,  1.3276,  0.5633],\n",
      "        [ 0.8740, -0.6457, -0.0094,  ..., -0.1396,  2.6263,  1.1143],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "tensor([8.0372e-21, 1.1497e-17, 1.7191e-02, 6.1501e-02, 2.5552e-17])\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Are you introverted?\"\n",
    "feature_indices = [53912]\n",
    "feature_values = [-10] \n",
    "tokens_id, tokens_str, activation_cache = process_input_hf(model, auto_tokenizer, prompt)\n",
    "print(\"Tokens ID (AutoTokenizer):\", tokens_id)\n",
    "print(\"Tokens String (AutoTokenizer):\", tokens_str)\n",
    "print(len(activation_cache))\n",
    "activation = get_activation_hf(activation_cache, layer_index)\n",
    "print(f\"resid_post_mlp for layer {layer_index}:\", activation.shape if activation is not None else \"None\")\n",
    "print(activation)\n",
    "\n",
    "latent_activations, recon_activations = encode_decode(autoencoder, activation)\n",
    "mse_error, error = calculate_error(activation, recon_activations)\n",
    "\n",
    "modified_recon_activations = feature_steering(autoencoder, activation, feature_indices, feature_values)\n",
    "print(\"orginal modified_recon_activations:\", modified_recon_activations)\n",
    "print(modified_recon_activations.shape)\n",
    "\n",
    "modified_recon_activations_new = modified_recon_activations + error\n",
    "print(\"modified_recon_activations + error:\", modified_recon_activations_new)\n",
    "\n",
    "mse_error_after, error_after = calculate_error(activation, modified_recon_activations_new)\n",
    "print(error_after)\n",
    "print(mse_error_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36c5b184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_gpt2_logits(model, tokens_id, tokenizer):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_id)\n",
    "        logits = outputs.logits\n",
    "        # 使用torch.argmax选出概率最高的token ids\n",
    "        predicted_ids = torch.argmin(logits, dim=-1)\n",
    "        # 解码生成的token ids\n",
    "        response = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "    return response, logits\n",
    "\n",
    "def chat_with_gpt2_top_k_candidates(model, tokens_id, tokenizer, top_k=10):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_id)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # 选择每个时间步上概率最高的top_k个token的logits\n",
    "        top_k_logits, top_k_indices = torch.topk(logits, k=top_k, dim=-1)\n",
    "        \n",
    "        # 解码每个token的索引以获取token字符串\n",
    "        top_k_tokens = [\n",
    "            [tokenizer.decode([idx]) for idx in indices[0]] for indices in top_k_indices\n",
    "        ]\n",
    "\n",
    "    for step in range(min(10, logits.shape[1])):  # 限制打印至最多前10个token\n",
    "            print(f\"Step {step + 1}:\")\n",
    "            for i in range(top_k):\n",
    "                token = tokenizer.decode([top_k_indices[0, step, i]])\n",
    "                logit = top_k_logits[0, step, i].item()\n",
    "                print(f\"  Candidate {i + 1}: {token} (Logit: {logit})\")\n",
    "            print(\"\\n\")\n",
    "        \n",
    "    return top_k_tokens, top_k_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7c0653",
   "metadata": {},
   "source": [
    "### Original Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e5454c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "  Candidate 1:  the (Logit: -29.919225692749023)\n",
      "  Candidate 2:  a (Logit: -30.52708625793457)\n",
      "  Candidate 3:  to (Logit: -30.872114181518555)\n",
      "  Candidate 4: , (Logit: -31.00540542602539)\n",
      "  Candidate 5: \n",
      " (Logit: -31.133556365966797)\n",
      "  Candidate 6:  you (Logit: -31.3023681640625)\n",
      "  Candidate 7: . (Logit: -31.325265884399414)\n",
      "  Candidate 8:  in (Logit: -31.418594360351562)\n",
      "  Candidate 9:  that (Logit: -31.46228790283203)\n",
      "  Candidate 10:  it (Logit: -31.574438095092773)\n",
      "\n",
      "\n",
      "Step 2:\n",
      "  Candidate 1:  a (Logit: -119.24163055419922)\n",
      "  Candidate 2:  sure (Logit: -119.3289794921875)\n",
      "  Candidate 3:  going (Logit: -119.41527557373047)\n",
      "  Candidate 4:  ready (Logit: -119.50484466552734)\n",
      "  Candidate 5:  looking (Logit: -120.21627807617188)\n",
      "  Candidate 6:  still (Logit: -120.2265396118164)\n",
      "  Candidate 7:  interested (Logit: -120.4596176147461)\n",
      "  Candidate 8:  using (Logit: -120.46378326416016)\n",
      "  Candidate 9:  in (Logit: -120.65441131591797)\n",
      "  Candidate 10:  worried (Logit: -120.6690444946289)\n",
      "\n",
      "\n",
      "Step 3:\n",
      "  Candidate 1: verted (Logit: -51.58854675292969)\n",
      "  Candidate 2: spective (Logit: -53.28206253051758)\n",
      "  Candidate 3: spect (Logit: -54.37642288208008)\n",
      "  Candidate 4: vert (Logit: -54.5172119140625)\n",
      "  Candidate 5: verts (Logit: -55.68467712402344)\n",
      "  Candidate 6: spection (Logit: -57.39292526245117)\n",
      "  Candidate 7: ? (Logit: -57.74125289916992)\n",
      "  Candidate 8: version (Logit: -57.999237060546875)\n",
      "  Candidate 9:  to (Logit: -58.08490753173828)\n",
      "  Candidate 10: - (Logit: -58.32606887817383)\n",
      "\n",
      "\n",
      "Step 4:\n",
      "  Candidate 1: ? (Logit: -74.06855010986328)\n",
      "  Candidate 2: , (Logit: -75.42339324951172)\n",
      "  Candidate 3: ?\" (Logit: -75.49243927001953)\n",
      "  Candidate 4:  or (Logit: -75.92665100097656)\n",
      "  Candidate 5:  and (Logit: -76.72173309326172)\n",
      "  Candidate 6:  enough (Logit: -77.468994140625)\n",
      "  Candidate 7:  yet (Logit: -77.6148452758789)\n",
      "  Candidate 8:  like (Logit: -78.0799789428711)\n",
      "  Candidate 9:  at (Logit: -78.23387908935547)\n",
      "  Candidate 10:  in (Logit: -78.2605209350586)\n",
      "\n",
      "\n",
      "Step 5:\n",
      "  Candidate 1: \n",
      " (Logit: -124.7516098022461)\n",
      "  Candidate 2:  Do (Logit: -126.13678741455078)\n",
      "  Candidate 3:  I (Logit: -126.33380889892578)\n",
      "  Candidate 4:  You (Logit: -126.39299774169922)\n",
      "  Candidate 5:  If (Logit: -126.39952850341797)\n",
      "  Candidate 6:  Are (Logit: -126.61884307861328)\n",
      "  Candidate 7:  What (Logit: -126.8447265625)\n",
      "  Candidate 8:  How (Logit: -126.89637756347656)\n",
      "  Candidate 9:  Then (Logit: -127.16068267822266)\n",
      "  Candidate 10:  Or (Logit: -127.1952133178711)\n",
      "\n",
      "\n",
      "Original Output: Are you introverted?\n",
      "\n",
      "I'm not. I'm not a big introvert. I'm not a big introvert. I'm not a big introvert. I'm not a big introvert. I'm not a big introvert. I'm not a big introvert. I'm not a big introvert. I'm not a big introvert. I'm not a big introvert. I'm not a big introvert. I'm not a big introvert. I\n",
      "logits type Output: �\u0017��ailability��\n"
     ]
    }
   ],
   "source": [
    "def chat_with_gpt2(model, tokens_id):\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(tokens_id, max_length=100, pad_token_id=auto_tokenizer.eos_token_id)\n",
    "    response = auto_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "response = chat_with_gpt2(model, tokens_id)\n",
    "response_l, logits = chat_with_gpt2_logits(model, tokens_id, auto_tokenizer)\n",
    "chat_with_gpt2_top_k_candidates(model, tokens_id, auto_tokenizer)\n",
    "print(\"Original Output:\", response)\n",
    "print(\"logits type Output:\", response_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f10ca",
   "metadata": {},
   "source": [
    "### Controlled Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "443b2cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n",
      "tensor([[[ 0.8602,  0.1959,  0.4753,  ..., -1.8215, -0.2200,  0.4110],\n",
      "         [-0.5927,  0.9537, -2.2768,  ..., -1.5256, -2.6308,  1.6227],\n",
      "         [ 2.7911, -3.7945, -2.8128,  ..., -3.8509,  0.1147, -1.6560],\n",
      "         [ 3.4625,  1.4570, -2.9760,  ..., -1.1660,  2.5300, -0.7437],\n",
      "         [ 1.3002,  1.2545, -1.8011,  ..., -3.2269, -0.4410,  2.9988]]])\n",
      "tensor([[ 8491,   345, 18951, 13658,    30]])\n",
      "['Are', 'Ġyou', 'Ġintro', 'verted', '?']\n"
     ]
    }
   ],
   "source": [
    "modified_activations = modified_recon_activations_new.unsqueeze(0)\n",
    "print(modified_activations.shape)\n",
    "print(modified_activations)\n",
    "print(tokens_id)\n",
    "print(tokens_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7b1ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "class CustomGPT2LMHeadModel(GPT2LMHeadModel):\n",
    "    def __init__(self, config, layer_to_modify=6, modified_activations=modified_activations):\n",
    "        super().__init__(config)\n",
    "        self.layer_to_modify = layer_to_modify\n",
    "        self.modified_activations = modified_activations\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # Transformer层的输出\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = transformer_outputs[0]\n",
    "\n",
    "\n",
    "        # 在第六层插入修改后的激活值\n",
    "        if self.modified_activations is not None and len(transformer_outputs.hidden_states) >= self.layer_to_modify:\n",
    "            # 从指定层开始使用提供的激活值进行修改\n",
    "            modified_states = self.modified_activations\n",
    "            for i in range(self.layer_to_modify, len(self.transformer.h)):\n",
    "                layer_module = self.transformer.h[i]\n",
    "                layer_outputs = layer_module(modified_states, attention_mask=None)  \n",
    "                modified_states = layer_outputs[0]\n",
    "\n",
    "            # 将最终输出设置为最后一层修改后的输出\n",
    "            hidden_states = modified_states\n",
    "\n",
    "        # 应用语言模型头\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Flatten the tokens\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithCrossAttentions(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "            cross_attentions=transformer_outputs.cross_attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "857ac1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomGPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = GPT2Config.from_pretrained('gpt2', output_hidden_states=True)\n",
    "model_modified = CustomGPT2LMHeadModel(config, layer_to_modify=6, modified_activations=modified_activations)\n",
    "#model_modified = CustomGPT2LMHeadModel.from_pretrained('gpt2', output_hidden_states=True)\n",
    "model_modified.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3dc2764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controlled output: Are you introverted?anutanutanutanutanutanutanutanutanutanutanutanutanutanutanut\n"
     ]
    }
   ],
   "source": [
    "generated_outputs = model_modified.generate(\n",
    "    tokens_id,\n",
    "    max_length=20,\n",
    "    output_hidden_states=True,\n",
    "    pad_token_id=auto_tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# 解码生成的文本\n",
    "generated_text = auto_tokenizer.decode(generated_outputs[0], skip_special_tokens=True, pad_token_id=auto_tokenizer.eos_token_id)\n",
    "print(\"Controlled output:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dda4a080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "  Candidate 1: Solution (Logit: 248.58131408691406)\n",
      "  Candidate 2:  AJ (Logit: 244.9464111328125)\n",
      "  Candidate 3:  Sab (Logit: 239.6248016357422)\n",
      "  Candidate 4:  con (Logit: 239.08999633789062)\n",
      "  Candidate 5:  rev (Logit: 237.34375)\n",
      "  Candidate 6:  gravity (Logit: 236.20005798339844)\n",
      "  Candidate 7:  exalted (Logit: 235.65975952148438)\n",
      "  Candidate 8: ithing (Logit: 229.19241333007812)\n",
      "  Candidate 9: 195 (Logit: 228.90213012695312)\n",
      "  Candidate 10:  Courtney (Logit: 228.195068359375)\n",
      "\n",
      "\n",
      "Step 2:\n",
      "  Candidate 1:  drops (Logit: 6.763707637786865)\n",
      "  Candidate 2:  embody (Logit: 6.657547950744629)\n",
      "  Candidate 3:  interruption (Logit: 6.513821601867676)\n",
      "  Candidate 4:  Words (Logit: 6.3390913009643555)\n",
      "  Candidate 5:  twisted (Logit: 6.283496379852295)\n",
      "  Candidate 6: (\\ (Logit: 6.263243675231934)\n",
      "  Candidate 7:  Riding (Logit: 6.154960632324219)\n",
      "  Candidate 8: ias (Logit: 6.122747421264648)\n",
      "  Candidate 9: hound (Logit: 6.067363739013672)\n",
      "  Candidate 10:  sinks (Logit: 6.004322052001953)\n",
      "\n",
      "\n",
      "Step 3:\n",
      "  Candidate 1:  residual (Logit: 7.391213417053223)\n",
      "  Candidate 2:  mega (Logit: 7.318572521209717)\n",
      "  Candidate 3:  diagnosed (Logit: 7.298359394073486)\n",
      "  Candidate 4:  inheritance (Logit: 7.169936656951904)\n",
      "  Candidate 5:  gossip (Logit: 6.808722496032715)\n",
      "  Candidate 6: ome (Logit: 6.767490386962891)\n",
      "  Candidate 7: � (Logit: 6.698105335235596)\n",
      "  Candidate 8:  Border (Logit: 6.644947528839111)\n",
      "  Candidate 9:  (_ (Logit: 6.62774658203125)\n",
      "  Candidate 10: df (Logit: 6.60477352142334)\n",
      "\n",
      "\n",
      "Step 4:\n",
      "  Candidate 1: trl (Logit: 7.899384498596191)\n",
      "  Candidate 2:  insofar (Logit: 7.378912925720215)\n",
      "  Candidate 3:  relating (Logit: 7.345961570739746)\n",
      "  Candidate 4:  Rochester (Logit: 7.246424674987793)\n",
      "  Candidate 5: Box (Logit: 7.223376750946045)\n",
      "  Candidate 6: opped (Logit: 7.086292266845703)\n",
      "  Candidate 7: reverse (Logit: 7.07386589050293)\n",
      "  Candidate 8: examination (Logit: 7.063406944274902)\n",
      "  Candidate 9: ithing (Logit: 7.02122688293457)\n",
      "  Candidate 10:  Stick (Logit: 6.923829078674316)\n",
      "\n",
      "\n",
      "Step 5:\n",
      "  Candidate 1: anut (Logit: 7.815317630767822)\n",
      "  Candidate 2:  overcl (Logit: 7.720680236816406)\n",
      "  Candidate 3: nutrition (Logit: 7.670173645019531)\n",
      "  Candidate 4: Opening (Logit: 7.26947021484375)\n",
      "  Candidate 5: Products (Logit: 7.10641622543335)\n",
      "  Candidate 6:  divided (Logit: 7.04020881652832)\n",
      "  Candidate 7: andre (Logit: 7.039178371429443)\n",
      "  Candidate 8:  residual (Logit: 6.983358383178711)\n",
      "  Candidate 9:  LSU (Logit: 6.967954158782959)\n",
      "  Candidate 10:  debating (Logit: 6.938361167907715)\n",
      "\n",
      "\n",
      "logits type Steered Output:  situations Epshawsingle chrom\n"
     ]
    }
   ],
   "source": [
    "generated_text_l, logits = chat_with_gpt2_logits(model_modified, tokens_id, auto_tokenizer)\n",
    "chat_with_gpt2_top_k_candidates(model_modified, tokens_id, auto_tokenizer)\n",
    "print(\"logits type Steered Output:\", generated_text_l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
