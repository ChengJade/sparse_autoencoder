{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:02:50.010183Z",
     "start_time": "2024-07-22T20:02:43.397665Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 设置环境变量\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('D:\\ComputerScience\\Research\\PRADA\\sparse_autoencoder')\n",
    "# os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "# 导入库\n",
    "import torch\n",
    "import blobfile as bf\n",
    "from experiments.utils import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2LMHeadModel, AutoTokenizer, GPT2Tokenizer, GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f3b4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_steering(autoencoder,x: torch.Tensor, feature_indices: list[int], feature_values: list[float]) -> torch.Tensor:\n",
    "    assert len(feature_indices) == len(feature_values), \"Feature indices and values must have the same length.\"\n",
    "    feature_values = [max(min(value, 10), -10) for value in feature_values]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 获取原始特征表示和信息\n",
    "        latents, info = autoencoder.encode(x)\n",
    "        # 修改特征表示\n",
    "        for index, value in zip(feature_indices, feature_values):\n",
    "            print(\"original:\", latents[:, index])\n",
    "            if value > 0:\n",
    "                latents[:, index] *= value\n",
    "            else:\n",
    "                latents[:, index] = latents[:, index] / abs(value)\n",
    "            print(\"Modified:\", latents[:, index])\n",
    "            print(f\"Feature {index} modified with {'+' if value >= 0 else ''}{value}\")\n",
    "        # 使用修改后的特征表示通过解码器生成重构输出\n",
    "        modified_output = autoencoder.decode(latents, info)\n",
    "    return modified_output\n",
    "\n",
    "def calculate_error(input_tensor, reconstructed_activations) -> torch.Tensor:\n",
    "    # 计算误差\n",
    "    error = input_tensor - reconstructed_activations\n",
    "    # 可以选择使用不同的误差度量方式，这里使用均方误差（MSE）\n",
    "    normalized_mse = (reconstructed_activations - input_tensor).pow(2).sum(dim=1) / (input_tensor).pow(2).sum(dim=1)\n",
    "    return normalized_mse, error\n",
    "\n",
    "\n",
    "def compare_activations(tensor1, tensor2):\n",
    "    difference = tensor1 - tensor2\n",
    "    print(\"Difference between tensors:\\n\", difference)\n",
    "\n",
    "    # 计算差异的统计信息\n",
    "    mean_diff = torch.mean(difference)\n",
    "    std_diff = torch.std(difference)\n",
    "    print(f\"Mean difference: {mean_diff.item()}\")\n",
    "    print(f\"Standard deviation of difference: {std_diff.item()}\")\n",
    "\n",
    "    # 可视化差异\n",
    "    difference_np = difference.numpy()\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.imshow(difference_np, cmap='coolwarm', aspect='auto')\n",
    "    plt.colorbar(label='Difference')\n",
    "    plt.title('Difference between Reconstructed Activations and Modified Output')\n",
    "    plt.xlabel('Feature Index')\n",
    "    plt.ylabel('Sample Index')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a5bb1aac7ca685e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:03:05.660715Z",
     "start_time": "2024-07-22T20:02:50.012200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current sae:az://openaipublic/sparse-autoencoder/gpt2-small/resid_post_mlp_v5_128k/autoencoders/6.pt\n"
     ]
    }
   ],
   "source": [
    "model, auto_tokenizer, device = load_model_hf(\"gpt2\")\n",
    "layer_index = 6\n",
    "location = \"resid_post_mlp\"\n",
    "autoencoder = load_autoencoder(location, layer_index, device, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8b23a7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: tensor([3.6126e-05, 3.3759e-02, 5.5583e-02, 5.5487e-02, 6.2234e-02])\n",
      "Error Tensor: tensor([[ 0.6114,  0.2868,  0.3416,  ...,  0.1430, -0.0825,  0.4055],\n",
      "        [ 0.5793,  0.8852,  0.1662,  ..., -1.2331, -0.4440, -0.5244],\n",
      "        [-0.1159, -0.9248,  0.0317,  ..., -0.9406,  0.9031, -0.0258],\n",
      "        [ 2.0144,  0.0149, -0.5361,  ...,  0.1203,  1.0314, -0.3879],\n",
      "        [ 0.0770,  0.7668, -0.1243,  ..., -1.3477,  0.1813,  1.1257]])\n",
      "torch.Size([5, 768])\n"
     ]
    }
   ],
   "source": [
    "# 计算误差\n",
    "mse_error, error = calculate_error(activation, recon_activations)\n",
    "print(\"MSE:\", mse_error)\n",
    "print(\"Error Tensor:\", error)\n",
    "print(error.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ebf18",
   "metadata": {},
   "source": [
    "# Activation Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "19b49ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens ID (AutoTokenizer): tensor([[ 8491,   345, 18951, 13658,    30]])\n",
      "Tokens String (AutoTokenizer): ['Are', 'Ġyou', 'Ġintro', 'verted', '?']\n",
      "13\n",
      "resid_post_mlp for layer 6: torch.Size([5, 768])\n",
      "tensor([[ 0.8602,  0.1959,  0.4753,  ..., -1.8215, -0.2200,  0.4110],\n",
      "        [-0.5927,  0.9537, -2.2768,  ..., -1.5256, -2.6308,  1.6227],\n",
      "        [ 3.2329, -4.1209, -2.8176,  ..., -3.9215,  1.4423, -1.0927],\n",
      "        [ 4.3366,  0.8113, -2.9855,  ..., -1.3056,  5.1563,  0.3707],\n",
      "        [ 1.3002,  1.2545, -1.8011,  ..., -3.2269, -0.4410,  2.9988]])\n",
      "original: tensor([0.0000, 0.0000, 4.0403, 7.6410, 0.0000])\n",
      "Modified: tensor([0.0000, 0.0000, 0.8081, 1.5282, 0.0000])\n",
      "Feature 53912 modified with -5\n",
      "orginal modified_recon_activations: tensor([[ 0.2488, -0.0908,  0.1337,  ..., -1.9644, -0.1375,  0.0055],\n",
      "        [-1.1720,  0.0685, -2.4429,  ..., -0.2925, -2.1868,  2.1471],\n",
      "        [ 2.9561, -2.9059, -2.8450,  ..., -2.9181, -0.6408, -1.5676],\n",
      "        [ 1.5453,  1.3703, -2.4410,  ..., -1.3018,  1.7904, -0.2320],\n",
      "        [ 1.2232,  0.4878, -1.6768,  ..., -1.8792, -0.6223,  1.8731]])\n",
      "torch.Size([5, 768])\n",
      "modified_recon_activations + error: tensor([[ 0.8602,  0.1959,  0.4753,  ..., -1.8215, -0.2200,  0.4110],\n",
      "        [-0.5927,  0.9537, -2.2768,  ..., -1.5256, -2.6308,  1.6227],\n",
      "        [ 2.8402, -3.8307, -2.8133,  ..., -3.8587,  0.2622, -1.5934],\n",
      "        [ 3.5597,  1.3852, -2.9771,  ..., -1.1815,  2.8218, -0.6199],\n",
      "        [ 1.3002,  1.2545, -1.8011,  ..., -3.2269, -0.4410,  2.9988]])\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.3927, -0.2901, -0.0042,  ..., -0.0627,  1.1801,  0.5007],\n",
      "        [ 0.7769, -0.5740, -0.0084,  ..., -0.1241,  2.3345,  0.9905],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "tensor([8.0372e-21, 1.1497e-17, 1.3583e-02, 4.8593e-02, 2.5552e-17])\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Are you introverted?\"\n",
    "feature_indices = [53912]\n",
    "feature_values = [-5] \n",
    "tokens_id, tokens_str, activation_cache = process_input_hf(model, auto_tokenizer, prompt)\n",
    "print(\"Tokens ID (AutoTokenizer):\", tokens_id)\n",
    "print(\"Tokens String (AutoTokenizer):\", tokens_str)\n",
    "print(len(activation_cache))\n",
    "activation = get_activation_hf(activation_cache, layer_index)\n",
    "print(f\"resid_post_mlp for layer {layer_index}:\", activation.shape if activation is not None else \"None\")\n",
    "print(activation)\n",
    "\n",
    "latent_activations, recon_activations = encode_decode(autoencoder, activation)\n",
    "mse_error, error = calculate_error(activation, recon_activations)\n",
    "\n",
    "modified_recon_activations = feature_steering(autoencoder, activation, feature_indices, feature_values)\n",
    "print(\"orginal modified_recon_activations:\", modified_recon_activations)\n",
    "print(modified_recon_activations.shape)\n",
    "\n",
    "modified_recon_activations_new = modified_recon_activations + error\n",
    "print(\"modified_recon_activations + error:\", modified_recon_activations_new)\n",
    "\n",
    "mse_error_after, error_after = calculate_error(activation, modified_recon_activations_new)\n",
    "print(error_after)\n",
    "print(mse_error_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7c0653",
   "metadata": {},
   "source": [
    "### Original Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "1e5454c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Output: Are you introverted?\n",
      "\n",
      "I'm not. I'm not a big introvert. I\n"
     ]
    }
   ],
   "source": [
    "def chat_with_gpt2(model, tokens_id):\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(tokens_id, max_length=20, pad_token_id=auto_tokenizer.eos_token_id)\n",
    "    response = auto_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "response = chat_with_gpt2(model, tokens_id)\n",
    "print(\"Original Output:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f10ca",
   "metadata": {},
   "source": [
    "### Controlled Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "443b2cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n",
      "tensor([[[ 0.8602,  0.1959,  0.4753,  ..., -1.8215, -0.2200,  0.4110],\n",
      "         [-0.5927,  0.9537, -2.2768,  ..., -1.5256, -2.6308,  1.6227],\n",
      "         [ 2.8402, -3.8307, -2.8133,  ..., -3.8587,  0.2622, -1.5934],\n",
      "         [ 3.5597,  1.3852, -2.9771,  ..., -1.1815,  2.8218, -0.6199],\n",
      "         [ 1.3002,  1.2545, -1.8011,  ..., -3.2269, -0.4410,  2.9988]]])\n",
      "tensor([[ 8491,   345, 18951, 13658,    30]])\n",
      "['Are', 'Ġyou', 'Ġintro', 'verted', '?']\n"
     ]
    }
   ],
   "source": [
    "modified_activations = modified_recon_activations_new.unsqueeze(0)\n",
    "print(modified_activations.shape)\n",
    "print(modified_activations)\n",
    "print(tokens_id)\n",
    "print(tokens_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "c7b1ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "class CustomGPT2LMHeadModel(GPT2LMHeadModel):\n",
    "    def __init__(self, config, layer_to_modify=6, modified_activations=modified_activations):\n",
    "        super().__init__(config)\n",
    "        self.layer_to_modify = layer_to_modify\n",
    "        self.modified_activations = modified_activations\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # Transformer层的输出\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = transformer_outputs[0]\n",
    "\n",
    "\n",
    "        # 在第六层插入修改后的激活值\n",
    "        if self.modified_activations is not None and len(transformer_outputs.hidden_states) >= self.layer_to_modify:\n",
    "            # 从指定层开始使用提供的激活值进行修改\n",
    "            modified_states = self.modified_activations\n",
    "            for i in range(self.layer_to_modify, len(self.transformer.h)):\n",
    "                layer_module = self.transformer.h[i]\n",
    "                layer_outputs = layer_module(modified_states, attention_mask=None)  \n",
    "                modified_states = layer_outputs[0]\n",
    "\n",
    "            # 将最终输出设置为最后一层修改后的输出\n",
    "            hidden_states = modified_states\n",
    "\n",
    "        # 应用语言模型头\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Flatten the tokens\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithCrossAttentions(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "            cross_attentions=transformer_outputs.cross_attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "857ac1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomGPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#config = GPT2Config.from_pretrained('gpt2')\n",
    "#model_modified = CustomGPT2LMHeadModel(config, layer_to_modify=6, modified_activations=modified_activations)\n",
    "model_modified = CustomGPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model_modified.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "c3dc2764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controlled output: Are you introverted? the the the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "generated_outputs = model_modified.generate(\n",
    "    tokens_id,\n",
    "    max_length=20,\n",
    "    output_hidden_states=True,\n",
    "    pad_token_id=auto_tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# 解码生成的文本\n",
    "generated_text = auto_tokenizer.decode(generated_outputs[0], skip_special_tokens=True)\n",
    "print(\"Controlled output:\", generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
